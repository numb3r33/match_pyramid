# AUTOGENERATED! DO NOT EDIT! File to edit: 00_data.ipynb (unless otherwise specified).

__all__ = ['NumericalizePair', 'Pad_Chunk_Pair', 'Undict', 'load_dataset']

# Cell
import torchtext

from inspect import signature
from fastai.text.all import *
from sklearn.feature_extraction.text import CountVectorizer


# Cell
class NumericalizePair(Numericalize):
    def encodes(self, o):
        return TensorText(tensor([self.o2i  [o_] for o_ in o['q1']])), TensorText(tensor([self.o2i  [o_] for o_ in o['q2']]))

# Cell
class Pad_Chunk_Pair(ItemTransform):
    "Pad `samples` by adding padding by chunks of size `seq_len`"
    def __init__(self, pad_idx=1, pad_first=True, seq_len=72,decode=True,**kwargs):
        store_attr('pad_idx, pad_first, seq_len,seq_len')
        super().__init__(**kwargs)
    def before_call(self, b):
        "Set `self.max_len` before encodes"
        xas, xbs = [], []
        for xs in b:
            xa, xb = xs[0]
            if isinstance(xa, TensorText):
                xas.append(xa.shape[0])
            if isinstance(xb, TensorText):
                xbs.append(xb.shape[0])

        self.max_len_a = max(xas)
        self.max_len_b = max(xbs)

    def __call__(self, b, **kwargs):
        self.before_call(b)
        return super().__call__(tuple(b), **kwargs)

    def encodes(self, batch):
        texts  = ([s[0][0] for s in batch], [s[0][1] for s in batch])
        labels = default_collate([s[1:] for s in batch])

        inps   = {}

        pa = default_collate([pad_chunk(ta,pad_idx=self.pad_idx, pad_first=self.pad_first, seq_len=self.seq_len, pad_len=self.max_len_a) for ta in texts[0]])
        pb = default_collate([pad_chunk(tb,pad_idx=self.pad_idx, pad_first=self.pad_first, seq_len=self.seq_len, pad_len=self.max_len_b) for tb in texts[1]])

        inps['pa'] = pa
        inps['pb'] = pb

        if len(labels):
            inps['labels'] = labels[0]

        res = (inps, )

        return res

# Cell
class Undict(Transform):
    def decodes(self, x:dict):
        if 'pa' in x and 'pb' in x: res = (x['pa'], x['pb'], x['labels'])
        return res

# Cell
def load_dataset():
    BASE_DIR      = Path('~/data/dl_nlp')
    RAW_DATA_PATH = BASE_DIR / 'data' / 'quodup'


    train       = pd.read_csv(RAW_DATA_PATH / 'train.csv')
    train       = train.sample(frac=1.)
    train.index = np.arange(len(train))

    # fill empty questions with ''
    train.loc[:, 'question1'] = train.question1.fillna('')
    train.loc[:, 'question2'] = train.question2.fillna('')

    return train